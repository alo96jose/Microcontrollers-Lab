{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Eh9yve14gUyD",
        "Gxk414PU3oy3",
        "NUDPvaJE1wRE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Google Colab levemente modificado y obtenido de:\n",
        "\n",
        "Gesture recognition tutorial\n",
        "\n",
        "    Sandeep Mistry - Arduino\n",
        "    Don Coleman - Chariot Solutions\n",
        "\n",
        "https://github.com/arduino/ArduinoTensorFlowLiteTutorials/"
      ],
      "metadata": {
        "id": "rJ4oR2eDOEzt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvDA8AK7QOq-"
      },
      "source": [
        "## Setup Python Environment\n",
        "\n",
        "The next cell sets up the dependencies in required for the notebook, run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2gs-PL4xDkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d267a5b6-f38f-48ac-abbb-b001ef6c8b38"
      },
      "source": [
        "# Setup environment\n",
        "!apt-get -qq install xxd\n",
        "!pip install pandas numpy matplotlib\n",
        "!pip install tensorflow==2.0.0-rc1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.0.0-rc1 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.0.0-rc1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lwkeshJk7dg"
      },
      "source": [
        "# Upload Data\n",
        "\n",
        "1. Open the panel on the left side of Colab by clicking on the __>__\n",
        "1. Select the files tab\n",
        "1. Drag `punch.csv` and `flex.csv` files from your computer to the tab to upload them into colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh9yve14gUyD"
      },
      "source": [
        "# Graph Data (optional)\n",
        "\n",
        "We'll graph the input files on two separate graphs, acceleration and gyroscope, as each data set has different units and scale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I65ukChEgyNp"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "filename = \"punch.csv\"\n",
        "\n",
        "df = pd.read_csv(\"/content/\" + filename)\n",
        "\n",
        "index = range(1, len(df['aX']) + 1)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
        "\n",
        "plt.plot(index, df['aX'], 'g.', label='x', linestyle='solid', marker=',')\n",
        "plt.plot(index, df['aY'], 'b.', label='y', linestyle='solid', marker=',')\n",
        "plt.plot(index, df['aZ'], 'r.', label='z', linestyle='solid', marker=',')\n",
        "plt.title(\"Acceleration\")\n",
        "plt.xlabel(\"Sample #\")\n",
        "plt.ylabel(\"Acceleration (G)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(index, df['gX'], 'g.', label='x', linestyle='solid', marker=',')\n",
        "plt.plot(index, df['gY'], 'b.', label='y', linestyle='solid', marker=',')\n",
        "plt.plot(index, df['gZ'], 'r.', label='z', linestyle='solid', marker=',')\n",
        "plt.title(\"Gyroscope\")\n",
        "plt.xlabel(\"Sample #\")\n",
        "plt.ylabel(\"Gyroscope (deg/sec)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSxUeYPNQbOg"
      },
      "source": [
        "# Train Neural Network\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxk414PU3oy3"
      },
      "source": [
        "## Parse and prepare the data\n",
        "\n",
        "The next cell parses the csv files and transforms them to a format that will be used to train the fully connected neural network.\n",
        "\n",
        "Update the `GESTURES` list with the gesture data you've collected in `.csv` format.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGChd1FAk5_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9acc90e-dd95-40e4-b639-9d53dbfea18a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "print(f\"TensorFlow version = {tf.__version__}\\n\")\n",
        "\n",
        "# Set a fixed random seed value, for reproducibility, this will allow us to get\n",
        "# the same random numbers each time the notebook is run\n",
        "SEED = 1337\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# the list of gestures that data is available for\n",
        "GESTURES = [\n",
        "    \"punch\",\n",
        "    \"flex\",\n",
        "    \"circulo\",\n",
        "]\n",
        "\n",
        "SAMPLES_PER_GESTURE = 119\n",
        "\n",
        "NUM_GESTURES = len(GESTURES)\n",
        "\n",
        "# create a one-hot encoded matrix that is used in the output\n",
        "ONE_HOT_ENCODED_GESTURES = np.eye(NUM_GESTURES)\n",
        "\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "# read each csv file and push an input and output\n",
        "for gesture_index in range(NUM_GESTURES):\n",
        "  gesture = GESTURES[gesture_index]\n",
        "  print(f\"Processing index {gesture_index} for gesture '{gesture}'.\")\n",
        "\n",
        "  output = ONE_HOT_ENCODED_GESTURES[gesture_index]\n",
        "\n",
        "  df = pd.read_csv(\"/content/\" + gesture + \".csv\")\n",
        "\n",
        "  # calculate the number of gesture recordings in the file\n",
        "  num_recordings = int(df.shape[0] / SAMPLES_PER_GESTURE)\n",
        "\n",
        "  print(f\"\\tThere are {num_recordings} recordings of the {gesture} gesture.\")\n",
        "\n",
        "  for i in range(num_recordings):\n",
        "    tensor = []\n",
        "    for j in range(SAMPLES_PER_GESTURE):\n",
        "      index = i * SAMPLES_PER_GESTURE + j\n",
        "      # normalize the input data, between 0 to 1:\n",
        "      # - acceleration is between: -4 to +4\n",
        "      # - gyroscope is between: -2000 to +2000\n",
        "      tensor += [\n",
        "          (df['aX'][index] + 4) / 8,\n",
        "          (df['aY'][index] + 4) / 8,\n",
        "          (df['aZ'][index] + 4) / 8,\n",
        "          (df['gX'][index] + 2000) / 4000,\n",
        "          (df['gY'][index] + 2000) / 4000,\n",
        "          (df['gZ'][index] + 2000) / 4000\n",
        "      ]\n",
        "\n",
        "    inputs.append(tensor)\n",
        "    outputs.append(output)\n",
        "\n",
        "# convert the list to numpy array\n",
        "inputs = np.array(inputs)\n",
        "outputs = np.array(outputs)\n",
        "\n",
        "print(\"Data set parsing and preparation complete.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version = 2.15.0\n",
            "\n",
            "Processing index 0 for gesture 'punch'.\n",
            "\tThere are 10 recordings of the punch gesture.\n",
            "Processing index 1 for gesture 'flex'.\n",
            "\tThere are 11 recordings of the flex gesture.\n",
            "Processing index 2 for gesture 'circulo'.\n",
            "\tThere are 10 recordings of the circulo gesture.\n",
            "Data set parsing and preparation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5_61831d5AM"
      },
      "source": [
        "## Randomize and split the input and output pairs for training\n",
        "\n",
        "Randomly split input and output pairs into sets of data: 60% for training, 20% for validation, and 20% for testing.\n",
        "\n",
        "  - the training set is used to train the model\n",
        "  - the validation set is used to measure how well the model is performing during training\n",
        "  - the testing set is used to test the model after training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfNEmUZMeIEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "736f4353-12b6-4da3-bca3-e1d935e07402"
      },
      "source": [
        "# Randomize the order of the inputs, so they can be evenly distributed for training, testing, and validation\n",
        "# https://stackoverflow.com/a/37710486/2020087\n",
        "num_inputs = len(inputs)\n",
        "randomize = np.arange(num_inputs)\n",
        "np.random.shuffle(randomize)\n",
        "\n",
        "# Swap the consecutive indexes (0, 1, 2, etc) with the randomized indexes\n",
        "inputs = inputs[randomize]\n",
        "outputs = outputs[randomize]\n",
        "\n",
        "# Split the recordings (group of samples) into three sets: training, testing and validation\n",
        "TRAIN_SPLIT = int(0.6 * num_inputs)\n",
        "TEST_SPLIT = int(0.2 * num_inputs + TRAIN_SPLIT)\n",
        "\n",
        "inputs_train, inputs_test, inputs_validate = np.split(inputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "outputs_train, outputs_test, outputs_validate = np.split(outputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "\n",
        "print(\"Data set randomization and splitting complete.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data set randomization and splitting complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9g2n41p24nR"
      },
      "source": [
        "## Build & Train the Model\n",
        "\n",
        "Build and train a [TensorFlow](https://www.tensorflow.org) model using the high-level [Keras](https://www.tensorflow.org/guide/keras) API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGNFa-lX24Qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a035578f-af94-4313-bff5-f2c23eb51d87"
      },
      "source": [
        "# build the model and train it\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(50, activation='relu')) # relu is used for performance\n",
        "model.add(tf.keras.layers.Dense(15, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(NUM_GESTURES, activation='softmax')) # softmax is used, because we only expect one gesture to occur per input\n",
        "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "history = model.fit(inputs_train, outputs_train, epochs=600, batch_size=1, validation_data=(inputs_validate, outputs_validate))\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/600\n",
            "18/18 [==============================] - 1s 15ms/step - loss: 0.2526 - mae: 0.4279 - val_loss: 0.2728 - val_mae: 0.4876\n",
            "Epoch 2/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2112 - mae: 0.4145 - val_loss: 0.2446 - val_mae: 0.4598\n",
            "Epoch 3/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2155 - mae: 0.4204 - val_loss: 0.3145 - val_mae: 0.5192\n",
            "Epoch 4/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.2079 - mae: 0.4155 - val_loss: 0.3513 - val_mae: 0.5429\n",
            "Epoch 5/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2079 - mae: 0.4047 - val_loss: 0.3312 - val_mae: 0.5312\n",
            "Epoch 6/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2006 - mae: 0.4052 - val_loss: 0.3462 - val_mae: 0.5386\n",
            "Epoch 7/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1974 - mae: 0.3906 - val_loss: 0.3088 - val_mae: 0.5109\n",
            "Epoch 8/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2026 - mae: 0.3993 - val_loss: 0.3069 - val_mae: 0.5140\n",
            "Epoch 9/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1910 - mae: 0.3953 - val_loss: 0.3412 - val_mae: 0.5357\n",
            "Epoch 10/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1960 - mae: 0.3951 - val_loss: 0.3742 - val_mae: 0.5567\n",
            "Epoch 11/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1942 - mae: 0.3927 - val_loss: 0.3609 - val_mae: 0.5483\n",
            "Epoch 12/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1852 - mae: 0.3782 - val_loss: 0.3361 - val_mae: 0.5334\n",
            "Epoch 13/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1837 - mae: 0.3774 - val_loss: 0.2888 - val_mae: 0.4998\n",
            "Epoch 14/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1739 - mae: 0.3770 - val_loss: 0.2849 - val_mae: 0.4959\n",
            "Epoch 15/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1668 - mae: 0.3573 - val_loss: 0.2680 - val_mae: 0.4834\n",
            "Epoch 16/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.1642 - mae: 0.3599 - val_loss: 0.2527 - val_mae: 0.4692\n",
            "Epoch 17/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1560 - mae: 0.3539 - val_loss: 0.2814 - val_mae: 0.4929\n",
            "Epoch 18/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.1480 - mae: 0.3406 - val_loss: 0.2645 - val_mae: 0.4789\n",
            "Epoch 19/600\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.1478 - mae: 0.3411 - val_loss: 0.2526 - val_mae: 0.4690\n",
            "Epoch 20/600\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.1343 - mae: 0.3217 - val_loss: 0.2558 - val_mae: 0.4673\n",
            "Epoch 21/600\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.1226 - mae: 0.3053 - val_loss: 0.2725 - val_mae: 0.4785\n",
            "Epoch 22/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1164 - mae: 0.2853 - val_loss: 0.2636 - val_mae: 0.4688\n",
            "Epoch 23/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1090 - mae: 0.2805 - val_loss: 0.2601 - val_mae: 0.4626\n",
            "Epoch 24/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1011 - mae: 0.2632 - val_loss: 0.2243 - val_mae: 0.4405\n",
            "Epoch 25/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0896 - mae: 0.2449 - val_loss: 0.2267 - val_mae: 0.4393\n",
            "Epoch 26/600\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.0860 - mae: 0.2467 - val_loss: 0.2632 - val_mae: 0.4694\n",
            "Epoch 27/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.0805 - mae: 0.2174 - val_loss: 0.2546 - val_mae: 0.4612\n",
            "Epoch 28/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.0728 - mae: 0.2174 - val_loss: 0.2624 - val_mae: 0.4528\n",
            "Epoch 29/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0672 - mae: 0.1895 - val_loss: 0.2494 - val_mae: 0.4527\n",
            "Epoch 30/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0619 - mae: 0.1803 - val_loss: 0.1646 - val_mae: 0.3749\n",
            "Epoch 31/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0555 - mae: 0.1878 - val_loss: 0.2252 - val_mae: 0.4321\n",
            "Epoch 32/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.0611 - mae: 0.1832 - val_loss: 0.1561 - val_mae: 0.3597\n",
            "Epoch 33/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0483 - mae: 0.1650 - val_loss: 0.1432 - val_mae: 0.3471\n",
            "Epoch 34/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.0500 - mae: 0.1640 - val_loss: 0.1285 - val_mae: 0.3269\n",
            "Epoch 35/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0416 - mae: 0.1548 - val_loss: 0.1762 - val_mae: 0.3674\n",
            "Epoch 36/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0380 - mae: 0.1514 - val_loss: 0.1793 - val_mae: 0.3787\n",
            "Epoch 37/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0401 - mae: 0.1377 - val_loss: 0.1181 - val_mae: 0.3068\n",
            "Epoch 38/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0316 - mae: 0.1310 - val_loss: 0.1130 - val_mae: 0.2976\n",
            "Epoch 39/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0305 - mae: 0.1294 - val_loss: 0.1423 - val_mae: 0.3372\n",
            "Epoch 40/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0301 - mae: 0.1237 - val_loss: 0.0918 - val_mae: 0.2692\n",
            "Epoch 41/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0192 - mae: 0.0883 - val_loss: 0.0756 - val_mae: 0.2137\n",
            "Epoch 42/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0223 - mae: 0.1013 - val_loss: 0.0976 - val_mae: 0.2585\n",
            "Epoch 43/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0184 - mae: 0.0999 - val_loss: 0.0798 - val_mae: 0.2231\n",
            "Epoch 44/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0186 - mae: 0.0913 - val_loss: 0.0788 - val_mae: 0.2427\n",
            "Epoch 45/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0158 - mae: 0.0873 - val_loss: 0.0747 - val_mae: 0.2354\n",
            "Epoch 46/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0122 - mae: 0.0794 - val_loss: 0.0853 - val_mae: 0.2515\n",
            "Epoch 47/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0159 - mae: 0.0842 - val_loss: 0.0697 - val_mae: 0.2173\n",
            "Epoch 48/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0079 - mae: 0.0649 - val_loss: 0.0821 - val_mae: 0.2446\n",
            "Epoch 49/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0615 - val_loss: 0.0780 - val_mae: 0.2373\n",
            "Epoch 50/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0631 - val_loss: 0.0563 - val_mae: 0.1966\n",
            "Epoch 51/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0099 - mae: 0.0633 - val_loss: 0.0487 - val_mae: 0.1707\n",
            "Epoch 52/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0062 - mae: 0.0525 - val_loss: 0.0692 - val_mae: 0.1708\n",
            "Epoch 53/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0055 - mae: 0.0510 - val_loss: 0.0592 - val_mae: 0.1439\n",
            "Epoch 54/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0061 - mae: 0.0526 - val_loss: 0.0519 - val_mae: 0.1636\n",
            "Epoch 55/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0048 - mae: 0.0438 - val_loss: 0.0412 - val_mae: 0.1496\n",
            "Epoch 56/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0028 - mae: 0.0368 - val_loss: 0.0440 - val_mae: 0.1555\n",
            "Epoch 57/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0034 - mae: 0.0379 - val_loss: 0.0402 - val_mae: 0.1380\n",
            "Epoch 58/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0017 - mae: 0.0283 - val_loss: 0.0424 - val_mae: 0.1538\n",
            "Epoch 59/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0023 - mae: 0.0306 - val_loss: 0.0349 - val_mae: 0.1281\n",
            "Epoch 60/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0031 - mae: 0.0300 - val_loss: 0.0443 - val_mae: 0.1198\n",
            "Epoch 61/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 7.7892e-04 - mae: 0.0188 - val_loss: 0.0606 - val_mae: 0.1297\n",
            "Epoch 62/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.0017 - mae: 0.0254 - val_loss: 0.0536 - val_mae: 0.1045\n",
            "Epoch 63/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 7.4318e-04 - mae: 0.0182 - val_loss: 0.0298 - val_mae: 0.1160\n",
            "Epoch 64/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 8.2166e-04 - mae: 0.0169 - val_loss: 0.0340 - val_mae: 0.1110\n",
            "Epoch 65/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 6.3259e-04 - mae: 0.0148 - val_loss: 0.0385 - val_mae: 0.1045\n",
            "Epoch 66/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 7.5850e-04 - mae: 0.0163 - val_loss: 0.0186 - val_mae: 0.0958\n",
            "Epoch 67/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 4.4960e-04 - mae: 0.0133 - val_loss: 0.0316 - val_mae: 0.1007\n",
            "Epoch 68/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.1503e-04 - mae: 0.0100 - val_loss: 0.0303 - val_mae: 0.1051\n",
            "Epoch 69/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 7.5911e-04 - mae: 0.0137 - val_loss: 0.0498 - val_mae: 0.0951\n",
            "Epoch 70/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.7170e-04 - mae: 0.0094 - val_loss: 0.0334 - val_mae: 0.0929\n",
            "Epoch 71/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.4178e-04 - mae: 0.0088 - val_loss: 0.0300 - val_mae: 0.0918\n",
            "Epoch 72/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.0721e-04 - mae: 0.0090 - val_loss: 0.0303 - val_mae: 0.0848\n",
            "Epoch 73/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0021e-04 - mae: 0.0069 - val_loss: 0.0326 - val_mae: 0.0902\n",
            "Epoch 74/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 9.9143e-05 - mae: 0.0065 - val_loss: 0.0416 - val_mae: 0.0856\n",
            "Epoch 75/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 6.4032e-05 - mae: 0.0055 - val_loss: 0.0356 - val_mae: 0.0798\n",
            "Epoch 76/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 4.9736e-05 - mae: 0.0054 - val_loss: 0.0310 - val_mae: 0.0831\n",
            "Epoch 77/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 4.7237e-05 - mae: 0.0048 - val_loss: 0.0317 - val_mae: 0.0789\n",
            "Epoch 78/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.7007e-05 - mae: 0.0041 - val_loss: 0.0391 - val_mae: 0.0798\n",
            "Epoch 79/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.1479e-05 - mae: 0.0043 - val_loss: 0.0324 - val_mae: 0.0778\n",
            "Epoch 80/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.1496e-05 - mae: 0.0035 - val_loss: 0.0298 - val_mae: 0.0790\n",
            "Epoch 81/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.4143e-05 - mae: 0.0036 - val_loss: 0.0302 - val_mae: 0.0777\n",
            "Epoch 82/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.0272e-05 - mae: 0.0031 - val_loss: 0.0378 - val_mae: 0.0784\n",
            "Epoch 83/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8267e-05 - mae: 0.0032 - val_loss: 0.0369 - val_mae: 0.0783\n",
            "Epoch 84/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.3978e-05 - mae: 0.0029 - val_loss: 0.0319 - val_mae: 0.0772\n",
            "Epoch 85/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.5013e-05 - mae: 0.0027 - val_loss: 0.0356 - val_mae: 0.0779\n",
            "Epoch 86/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.2171e-05 - mae: 0.0026 - val_loss: 0.0374 - val_mae: 0.0782\n",
            "Epoch 87/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.1489e-05 - mae: 0.0025 - val_loss: 0.0370 - val_mae: 0.0776\n",
            "Epoch 88/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0533e-05 - mae: 0.0024 - val_loss: 0.0333 - val_mae: 0.0754\n",
            "Epoch 89/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 8.9545e-06 - mae: 0.0023 - val_loss: 0.0336 - val_mae: 0.0776\n",
            "Epoch 90/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 9.5438e-06 - mae: 0.0022 - val_loss: 0.0349 - val_mae: 0.0750\n",
            "Epoch 91/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 8.5413e-06 - mae: 0.0022 - val_loss: 0.0346 - val_mae: 0.0748\n",
            "Epoch 92/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 7.9336e-06 - mae: 0.0021 - val_loss: 0.0354 - val_mae: 0.0759\n",
            "Epoch 93/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 7.3264e-06 - mae: 0.0020 - val_loss: 0.0369 - val_mae: 0.0770\n",
            "Epoch 94/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 6.4288e-06 - mae: 0.0020 - val_loss: 0.0348 - val_mae: 0.0767\n",
            "Epoch 95/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 6.5592e-06 - mae: 0.0018 - val_loss: 0.0363 - val_mae: 0.0757\n",
            "Epoch 96/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 6.2738e-06 - mae: 0.0019 - val_loss: 0.0363 - val_mae: 0.0761\n",
            "Epoch 97/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 5.8902e-06 - mae: 0.0019 - val_loss: 0.0354 - val_mae: 0.0757\n",
            "Epoch 98/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.4631e-06 - mae: 0.0018 - val_loss: 0.0344 - val_mae: 0.0752\n",
            "Epoch 99/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 5.5006e-06 - mae: 0.0018 - val_loss: 0.0348 - val_mae: 0.0750\n",
            "Epoch 100/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 4.9360e-06 - mae: 0.0017 - val_loss: 0.0344 - val_mae: 0.0752\n",
            "Epoch 101/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.0270e-06 - mae: 0.0017 - val_loss: 0.0350 - val_mae: 0.0748\n",
            "Epoch 102/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 4.6715e-06 - mae: 0.0016 - val_loss: 0.0354 - val_mae: 0.0745\n",
            "Epoch 103/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 4.3925e-06 - mae: 0.0016 - val_loss: 0.0352 - val_mae: 0.0750\n",
            "Epoch 104/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 4.2733e-06 - mae: 0.0016 - val_loss: 0.0349 - val_mae: 0.0745\n",
            "Epoch 105/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 4.1354e-06 - mae: 0.0016 - val_loss: 0.0346 - val_mae: 0.0742\n",
            "Epoch 106/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.9157e-06 - mae: 0.0015 - val_loss: 0.0353 - val_mae: 0.0742\n",
            "Epoch 107/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.8186e-06 - mae: 0.0015 - val_loss: 0.0353 - val_mae: 0.0743\n",
            "Epoch 108/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.6968e-06 - mae: 0.0015 - val_loss: 0.0353 - val_mae: 0.0743\n",
            "Epoch 109/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.5763e-06 - mae: 0.0015 - val_loss: 0.0352 - val_mae: 0.0741\n",
            "Epoch 110/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.4601e-06 - mae: 0.0014 - val_loss: 0.0353 - val_mae: 0.0741\n",
            "Epoch 111/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.3265e-06 - mae: 0.0014 - val_loss: 0.0354 - val_mae: 0.0744\n",
            "Epoch 112/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.2302e-06 - mae: 0.0014 - val_loss: 0.0351 - val_mae: 0.0740\n",
            "Epoch 113/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.1381e-06 - mae: 0.0013 - val_loss: 0.0354 - val_mae: 0.0740\n",
            "Epoch 114/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.9198e-06 - mae: 0.0013 - val_loss: 0.0353 - val_mae: 0.0744\n",
            "Epoch 115/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.9871e-06 - mae: 0.0013 - val_loss: 0.0356 - val_mae: 0.0742\n",
            "Epoch 116/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.8747e-06 - mae: 0.0013 - val_loss: 0.0354 - val_mae: 0.0740\n",
            "Epoch 117/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.7546e-06 - mae: 0.0013 - val_loss: 0.0350 - val_mae: 0.0738\n",
            "Epoch 118/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.7309e-06 - mae: 0.0013 - val_loss: 0.0354 - val_mae: 0.0741\n",
            "Epoch 119/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.6507e-06 - mae: 0.0013 - val_loss: 0.0353 - val_mae: 0.0738\n",
            "Epoch 120/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.5221e-06 - mae: 0.0012 - val_loss: 0.0351 - val_mae: 0.0739\n",
            "Epoch 121/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.5240e-06 - mae: 0.0012 - val_loss: 0.0355 - val_mae: 0.0737\n",
            "Epoch 122/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.4443e-06 - mae: 0.0012 - val_loss: 0.0354 - val_mae: 0.0736\n",
            "Epoch 123/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.3962e-06 - mae: 0.0012 - val_loss: 0.0356 - val_mae: 0.0738\n",
            "Epoch 124/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.3432e-06 - mae: 0.0012 - val_loss: 0.0358 - val_mae: 0.0739\n",
            "Epoch 125/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.2386e-06 - mae: 0.0012 - val_loss: 0.0356 - val_mae: 0.0740\n",
            "Epoch 126/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.2342e-06 - mae: 0.0011 - val_loss: 0.0359 - val_mae: 0.0739\n",
            "Epoch 127/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.1891e-06 - mae: 0.0011 - val_loss: 0.0359 - val_mae: 0.0740\n",
            "Epoch 128/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.1096e-06 - mae: 0.0011 - val_loss: 0.0361 - val_mae: 0.0743\n",
            "Epoch 129/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.1046e-06 - mae: 0.0011 - val_loss: 0.0361 - val_mae: 0.0740\n",
            "Epoch 130/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.0502e-06 - mae: 0.0011 - val_loss: 0.0360 - val_mae: 0.0740\n",
            "Epoch 131/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.0145e-06 - mae: 0.0011 - val_loss: 0.0361 - val_mae: 0.0738\n",
            "Epoch 132/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.9785e-06 - mae: 0.0011 - val_loss: 0.0362 - val_mae: 0.0739\n",
            "Epoch 133/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.9398e-06 - mae: 0.0011 - val_loss: 0.0362 - val_mae: 0.0739\n",
            "Epoch 134/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.9041e-06 - mae: 0.0011 - val_loss: 0.0360 - val_mae: 0.0737\n",
            "Epoch 135/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.8613e-06 - mae: 0.0011 - val_loss: 0.0360 - val_mae: 0.0738\n",
            "Epoch 136/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.8283e-06 - mae: 0.0010 - val_loss: 0.0360 - val_mae: 0.0737\n",
            "Epoch 137/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.7930e-06 - mae: 0.0010 - val_loss: 0.0359 - val_mae: 0.0737\n",
            "Epoch 138/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.7536e-06 - mae: 0.0010 - val_loss: 0.0357 - val_mae: 0.0736\n",
            "Epoch 139/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.7417e-06 - mae: 0.0010 - val_loss: 0.0358 - val_mae: 0.0735\n",
            "Epoch 140/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.6977e-06 - mae: 9.8853e-04 - val_loss: 0.0359 - val_mae: 0.0734\n",
            "Epoch 141/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.6756e-06 - mae: 0.0010 - val_loss: 0.0358 - val_mae: 0.0734\n",
            "Epoch 142/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.6486e-06 - mae: 9.8817e-04 - val_loss: 0.0358 - val_mae: 0.0733\n",
            "Epoch 143/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.6246e-06 - mae: 9.6613e-04 - val_loss: 0.0360 - val_mae: 0.0734\n",
            "Epoch 144/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.5704e-06 - mae: 9.6996e-04 - val_loss: 0.0360 - val_mae: 0.0737\n",
            "Epoch 145/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.5708e-06 - mae: 9.6137e-04 - val_loss: 0.0361 - val_mae: 0.0737\n",
            "Epoch 146/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.5462e-06 - mae: 9.3720e-04 - val_loss: 0.0362 - val_mae: 0.0736\n",
            "Epoch 147/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.5253e-06 - mae: 9.4711e-04 - val_loss: 0.0363 - val_mae: 0.0736\n",
            "Epoch 148/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.5004e-06 - mae: 9.4210e-04 - val_loss: 0.0363 - val_mae: 0.0736\n",
            "Epoch 149/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.4768e-06 - mae: 9.3921e-04 - val_loss: 0.0363 - val_mae: 0.0736\n",
            "Epoch 150/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.4507e-06 - mae: 9.2890e-04 - val_loss: 0.0362 - val_mae: 0.0736\n",
            "Epoch 151/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.4286e-06 - mae: 9.2243e-04 - val_loss: 0.0361 - val_mae: 0.0735\n",
            "Epoch 152/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.4136e-06 - mae: 8.9890e-04 - val_loss: 0.0361 - val_mae: 0.0734\n",
            "Epoch 153/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.3923e-06 - mae: 8.9526e-04 - val_loss: 0.0362 - val_mae: 0.0734\n",
            "Epoch 154/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.3718e-06 - mae: 8.9504e-04 - val_loss: 0.0363 - val_mae: 0.0734\n",
            "Epoch 155/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.3590e-06 - mae: 9.0117e-04 - val_loss: 0.0363 - val_mae: 0.0735\n",
            "Epoch 156/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.3336e-06 - mae: 8.9213e-04 - val_loss: 0.0363 - val_mae: 0.0735\n",
            "Epoch 157/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.3219e-06 - mae: 8.8333e-04 - val_loss: 0.0364 - val_mae: 0.0736\n",
            "Epoch 158/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.3005e-06 - mae: 8.6793e-04 - val_loss: 0.0364 - val_mae: 0.0735\n",
            "Epoch 159/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.2853e-06 - mae: 8.7695e-04 - val_loss: 0.0364 - val_mae: 0.0735\n",
            "Epoch 160/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.2678e-06 - mae: 8.6649e-04 - val_loss: 0.0363 - val_mae: 0.0734\n",
            "Epoch 161/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.2520e-06 - mae: 8.6610e-04 - val_loss: 0.0364 - val_mae: 0.0735\n",
            "Epoch 162/600\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 1.2346e-06 - mae: 8.5877e-04 - val_loss: 0.0364 - val_mae: 0.0735\n",
            "Epoch 163/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.2224e-06 - mae: 8.4270e-04 - val_loss: 0.0364 - val_mae: 0.0734\n",
            "Epoch 164/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.2073e-06 - mae: 8.4187e-04 - val_loss: 0.0364 - val_mae: 0.0734\n",
            "Epoch 165/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1848e-06 - mae: 8.4389e-04 - val_loss: 0.0364 - val_mae: 0.0734\n",
            "Epoch 166/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.1743e-06 - mae: 8.1323e-04 - val_loss: 0.0364 - val_mae: 0.0734\n",
            "Epoch 167/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.1610e-06 - mae: 8.3336e-04 - val_loss: 0.0364 - val_mae: 0.0734\n",
            "Epoch 168/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.1517e-06 - mae: 8.1884e-04 - val_loss: 0.0365 - val_mae: 0.0734\n",
            "Epoch 169/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.1372e-06 - mae: 8.1651e-04 - val_loss: 0.0365 - val_mae: 0.0734\n",
            "Epoch 170/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.1218e-06 - mae: 8.1976e-04 - val_loss: 0.0365 - val_mae: 0.0734\n",
            "Epoch 171/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.1094e-06 - mae: 8.0110e-04 - val_loss: 0.0366 - val_mae: 0.0734\n",
            "Epoch 172/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0981e-06 - mae: 8.0399e-04 - val_loss: 0.0366 - val_mae: 0.0734\n",
            "Epoch 173/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0839e-06 - mae: 8.0472e-04 - val_loss: 0.0366 - val_mae: 0.0734\n",
            "Epoch 174/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0737e-06 - mae: 7.9652e-04 - val_loss: 0.0366 - val_mae: 0.0734\n",
            "Epoch 175/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0628e-06 - mae: 7.9082e-04 - val_loss: 0.0367 - val_mae: 0.0735\n",
            "Epoch 176/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0512e-06 - mae: 7.8711e-04 - val_loss: 0.0367 - val_mae: 0.0735\n",
            "Epoch 177/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0395e-06 - mae: 7.7823e-04 - val_loss: 0.0366 - val_mae: 0.0734\n",
            "Epoch 178/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0280e-06 - mae: 7.8204e-04 - val_loss: 0.0367 - val_mae: 0.0734\n",
            "Epoch 179/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0169e-06 - mae: 7.6239e-04 - val_loss: 0.0367 - val_mae: 0.0734\n",
            "Epoch 180/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0062e-06 - mae: 7.6588e-04 - val_loss: 0.0367 - val_mae: 0.0733\n",
            "Epoch 181/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 9.9655e-07 - mae: 7.6882e-04 - val_loss: 0.0367 - val_mae: 0.0733\n",
            "Epoch 182/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 9.8456e-07 - mae: 7.6674e-04 - val_loss: 0.0367 - val_mae: 0.0733\n",
            "Epoch 183/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 9.7691e-07 - mae: 7.5740e-04 - val_loss: 0.0367 - val_mae: 0.0734\n",
            "Epoch 184/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 9.6626e-07 - mae: 7.4900e-04 - val_loss: 0.0367 - val_mae: 0.0733\n",
            "Epoch 185/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 9.5765e-07 - mae: 7.4998e-04 - val_loss: 0.0367 - val_mae: 0.0733\n",
            "Epoch 186/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 9.4015e-07 - mae: 7.5014e-04 - val_loss: 0.0366 - val_mae: 0.0733\n",
            "Epoch 187/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 9.3838e-07 - mae: 7.4694e-04 - val_loss: 0.0366 - val_mae: 0.0733\n",
            "Epoch 188/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 9.2845e-07 - mae: 7.3181e-04 - val_loss: 0.0367 - val_mae: 0.0732\n",
            "Epoch 189/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 9.2039e-07 - mae: 7.4071e-04 - val_loss: 0.0367 - val_mae: 0.0732\n",
            "Epoch 190/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 9.1197e-07 - mae: 7.2698e-04 - val_loss: 0.0367 - val_mae: 0.0732\n",
            "Epoch 191/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 9.0444e-07 - mae: 7.2918e-04 - val_loss: 0.0368 - val_mae: 0.0733\n",
            "Epoch 192/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 8.9589e-07 - mae: 7.2740e-04 - val_loss: 0.0368 - val_mae: 0.0733\n",
            "Epoch 193/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 8.8491e-07 - mae: 7.1459e-04 - val_loss: 0.0368 - val_mae: 0.0732\n",
            "Epoch 194/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 8.7877e-07 - mae: 7.2229e-04 - val_loss: 0.0368 - val_mae: 0.0732\n",
            "Epoch 195/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 8.7106e-07 - mae: 7.1092e-04 - val_loss: 0.0368 - val_mae: 0.0732\n",
            "Epoch 196/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 8.5786e-07 - mae: 7.1468e-04 - val_loss: 0.0367 - val_mae: 0.0732\n",
            "Epoch 197/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 8.5602e-07 - mae: 7.0825e-04 - val_loss: 0.0368 - val_mae: 0.0732\n",
            "Epoch 198/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 8.4572e-07 - mae: 7.1135e-04 - val_loss: 0.0368 - val_mae: 0.0732\n",
            "Epoch 199/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 8.4014e-07 - mae: 6.9689e-04 - val_loss: 0.0368 - val_mae: 0.0732\n",
            "Epoch 200/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 8.3276e-07 - mae: 6.9679e-04 - val_loss: 0.0368 - val_mae: 0.0732\n",
            "Epoch 201/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 8.2672e-07 - mae: 6.9473e-04 - val_loss: 0.0368 - val_mae: 0.0732\n",
            "Epoch 202/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 8.1378e-07 - mae: 6.9480e-04 - val_loss: 0.0367 - val_mae: 0.0731\n",
            "Epoch 203/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 8.1334e-07 - mae: 6.9064e-04 - val_loss: 0.0368 - val_mae: 0.0731\n",
            "Epoch 204/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 8.0567e-07 - mae: 6.8321e-04 - val_loss: 0.0367 - val_mae: 0.0731\n",
            "Epoch 205/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 7.9980e-07 - mae: 6.8407e-04 - val_loss: 0.0368 - val_mae: 0.0731\n",
            "Epoch 206/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 7.9277e-07 - mae: 6.8348e-04 - val_loss: 0.0368 - val_mae: 0.0731\n",
            "Epoch 207/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 7.8637e-07 - mae: 6.7546e-04 - val_loss: 0.0368 - val_mae: 0.0731\n",
            "Epoch 208/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 7.8024e-07 - mae: 6.7599e-04 - val_loss: 0.0369 - val_mae: 0.0731\n",
            "Epoch 209/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 7.7320e-07 - mae: 6.6984e-04 - val_loss: 0.0369 - val_mae: 0.0731\n",
            "Epoch 210/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 7.6760e-07 - mae: 6.7335e-04 - val_loss: 0.0369 - val_mae: 0.0731\n",
            "Epoch 211/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 7.6129e-07 - mae: 6.6857e-04 - val_loss: 0.0369 - val_mae: 0.0731\n",
            "Epoch 212/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 7.5514e-07 - mae: 6.6861e-04 - val_loss: 0.0369 - val_mae: 0.0731\n",
            "Epoch 213/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 7.4843e-07 - mae: 6.5765e-04 - val_loss: 0.0370 - val_mae: 0.0731\n",
            "Epoch 214/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 7.4308e-07 - mae: 6.5814e-04 - val_loss: 0.0370 - val_mae: 0.0731\n",
            "Epoch 215/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 7.3409e-07 - mae: 6.6139e-04 - val_loss: 0.0369 - val_mae: 0.0731\n",
            "Epoch 216/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 7.3150e-07 - mae: 6.5088e-04 - val_loss: 0.0370 - val_mae: 0.0731\n",
            "Epoch 217/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 7.2675e-07 - mae: 6.5276e-04 - val_loss: 0.0369 - val_mae: 0.0731\n",
            "Epoch 218/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 7.2112e-07 - mae: 6.4809e-04 - val_loss: 0.0370 - val_mae: 0.0731\n",
            "Epoch 219/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 7.1582e-07 - mae: 6.4773e-04 - val_loss: 0.0370 - val_mae: 0.0731\n",
            "Epoch 220/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 7.0988e-07 - mae: 6.4854e-04 - val_loss: 0.0370 - val_mae: 0.0731\n",
            "Epoch 221/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 7.0352e-07 - mae: 6.3820e-04 - val_loss: 0.0370 - val_mae: 0.0731\n",
            "Epoch 222/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 6.9820e-07 - mae: 6.4526e-04 - val_loss: 0.0370 - val_mae: 0.0731\n",
            "Epoch 223/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 6.9477e-07 - mae: 6.3935e-04 - val_loss: 0.0370 - val_mae: 0.0731\n",
            "Epoch 224/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 6.8921e-07 - mae: 6.3929e-04 - val_loss: 0.0370 - val_mae: 0.0731\n",
            "Epoch 225/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 6.8471e-07 - mae: 6.3154e-04 - val_loss: 0.0370 - val_mae: 0.0731\n",
            "Epoch 226/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 6.7987e-07 - mae: 6.2908e-04 - val_loss: 0.0370 - val_mae: 0.0731\n",
            "Epoch 227/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 6.7120e-07 - mae: 6.3117e-04 - val_loss: 0.0370 - val_mae: 0.0730\n",
            "Epoch 228/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 6.7044e-07 - mae: 6.2706e-04 - val_loss: 0.0370 - val_mae: 0.0730\n",
            "Epoch 229/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 6.6564e-07 - mae: 6.2122e-04 - val_loss: 0.0370 - val_mae: 0.0730\n",
            "Epoch 230/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 6.6119e-07 - mae: 6.2282e-04 - val_loss: 0.0370 - val_mae: 0.0730\n",
            "Epoch 231/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 6.5668e-07 - mae: 6.2040e-04 - val_loss: 0.0370 - val_mae: 0.0730\n",
            "Epoch 232/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 6.5200e-07 - mae: 6.1903e-04 - val_loss: 0.0370 - val_mae: 0.0730\n",
            "Epoch 233/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 6.4748e-07 - mae: 6.1756e-04 - val_loss: 0.0370 - val_mae: 0.0730\n",
            "Epoch 234/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 6.4326e-07 - mae: 6.1425e-04 - val_loss: 0.0370 - val_mae: 0.0730\n",
            "Epoch 235/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 6.3868e-07 - mae: 6.0993e-04 - val_loss: 0.0370 - val_mae: 0.0730\n",
            "Epoch 236/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 6.3451e-07 - mae: 6.0697e-04 - val_loss: 0.0370 - val_mae: 0.0730\n",
            "Epoch 237/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 6.3019e-07 - mae: 6.0835e-04 - val_loss: 0.0370 - val_mae: 0.0730\n",
            "Epoch 238/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 6.2601e-07 - mae: 6.0650e-04 - val_loss: 0.0371 - val_mae: 0.0730\n",
            "Epoch 239/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 6.2173e-07 - mae: 6.0283e-04 - val_loss: 0.0371 - val_mae: 0.0730\n",
            "Epoch 240/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 6.1613e-07 - mae: 6.0064e-04 - val_loss: 0.0370 - val_mae: 0.0729\n",
            "Epoch 241/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 6.1427e-07 - mae: 5.9889e-04 - val_loss: 0.0370 - val_mae: 0.0729\n",
            "Epoch 242/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 6.0991e-07 - mae: 5.9883e-04 - val_loss: 0.0370 - val_mae: 0.0729\n",
            "Epoch 243/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 6.0633e-07 - mae: 5.9497e-04 - val_loss: 0.0371 - val_mae: 0.0729\n",
            "Epoch 244/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 6.0232e-07 - mae: 5.9141e-04 - val_loss: 0.0371 - val_mae: 0.0729\n",
            "Epoch 245/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.9843e-07 - mae: 5.9147e-04 - val_loss: 0.0371 - val_mae: 0.0729\n",
            "Epoch 246/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 5.9406e-07 - mae: 5.9183e-04 - val_loss: 0.0371 - val_mae: 0.0730\n",
            "Epoch 247/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.9054e-07 - mae: 5.8488e-04 - val_loss: 0.0371 - val_mae: 0.0729\n",
            "Epoch 248/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 5.8712e-07 - mae: 5.8851e-04 - val_loss: 0.0371 - val_mae: 0.0729\n",
            "Epoch 249/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 5.8294e-07 - mae: 5.8693e-04 - val_loss: 0.0371 - val_mae: 0.0729\n",
            "Epoch 250/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.7773e-07 - mae: 5.8505e-04 - val_loss: 0.0370 - val_mae: 0.0728\n",
            "Epoch 251/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.7610e-07 - mae: 5.7611e-04 - val_loss: 0.0370 - val_mae: 0.0728\n",
            "Epoch 252/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.7326e-07 - mae: 5.7762e-04 - val_loss: 0.0370 - val_mae: 0.0728\n",
            "Epoch 253/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.6896e-07 - mae: 5.8010e-04 - val_loss: 0.0370 - val_mae: 0.0728\n",
            "Epoch 254/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 5.6603e-07 - mae: 5.7657e-04 - val_loss: 0.0371 - val_mae: 0.0728\n",
            "Epoch 255/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 5.6250e-07 - mae: 5.7110e-04 - val_loss: 0.0371 - val_mae: 0.0728\n",
            "Epoch 256/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.5741e-07 - mae: 5.7340e-04 - val_loss: 0.0371 - val_mae: 0.0728\n",
            "Epoch 257/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.5611e-07 - mae: 5.7185e-04 - val_loss: 0.0371 - val_mae: 0.0728\n",
            "Epoch 258/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.5307e-07 - mae: 5.6770e-04 - val_loss: 0.0371 - val_mae: 0.0728\n",
            "Epoch 259/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.4981e-07 - mae: 5.6746e-04 - val_loss: 0.0371 - val_mae: 0.0728\n",
            "Epoch 260/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.4672e-07 - mae: 5.6356e-04 - val_loss: 0.0371 - val_mae: 0.0728\n",
            "Epoch 261/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 5.4307e-07 - mae: 5.6576e-04 - val_loss: 0.0371 - val_mae: 0.0728\n",
            "Epoch 262/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 5.3962e-07 - mae: 5.5862e-04 - val_loss: 0.0371 - val_mae: 0.0728\n",
            "Epoch 263/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 5.3705e-07 - mae: 5.5879e-04 - val_loss: 0.0371 - val_mae: 0.0728\n",
            "Epoch 264/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 5.3375e-07 - mae: 5.5731e-04 - val_loss: 0.0371 - val_mae: 0.0728\n",
            "Epoch 265/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.3010e-07 - mae: 5.5549e-04 - val_loss: 0.0371 - val_mae: 0.0728\n",
            "Epoch 266/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 5.2795e-07 - mae: 5.5543e-04 - val_loss: 0.0371 - val_mae: 0.0728\n",
            "Epoch 267/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 5.2413e-07 - mae: 5.5677e-04 - val_loss: 0.0371 - val_mae: 0.0728\n",
            "Epoch 268/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.2229e-07 - mae: 5.5084e-04 - val_loss: 0.0371 - val_mae: 0.0728\n",
            "Epoch 269/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 5.1921e-07 - mae: 5.4837e-04 - val_loss: 0.0372 - val_mae: 0.0728\n",
            "Epoch 270/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 5.1618e-07 - mae: 5.5216e-04 - val_loss: 0.0372 - val_mae: 0.0728\n",
            "Epoch 271/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.1309e-07 - mae: 5.5000e-04 - val_loss: 0.0372 - val_mae: 0.0728\n",
            "Epoch 272/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 5.1072e-07 - mae: 5.4735e-04 - val_loss: 0.0372 - val_mae: 0.0728\n",
            "Epoch 273/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.0808e-07 - mae: 5.4334e-04 - val_loss: 0.0372 - val_mae: 0.0728\n",
            "Epoch 274/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 5.0504e-07 - mae: 5.4445e-04 - val_loss: 0.0372 - val_mae: 0.0728\n",
            "Epoch 275/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 5.0213e-07 - mae: 5.3893e-04 - val_loss: 0.0372 - val_mae: 0.0728\n",
            "Epoch 276/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 4.9872e-07 - mae: 5.4029e-04 - val_loss: 0.0372 - val_mae: 0.0728\n",
            "Epoch 277/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 4.9545e-07 - mae: 5.3841e-04 - val_loss: 0.0371 - val_mae: 0.0727\n",
            "Epoch 278/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 4.9494e-07 - mae: 5.3721e-04 - val_loss: 0.0372 - val_mae: 0.0727\n",
            "Epoch 279/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 4.9243e-07 - mae: 5.3447e-04 - val_loss: 0.0372 - val_mae: 0.0727\n",
            "Epoch 280/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 4.8966e-07 - mae: 5.3436e-04 - val_loss: 0.0372 - val_mae: 0.0728\n",
            "Epoch 281/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 4.8714e-07 - mae: 5.3094e-04 - val_loss: 0.0372 - val_mae: 0.0728\n",
            "Epoch 282/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 4.8445e-07 - mae: 5.2897e-04 - val_loss: 0.0372 - val_mae: 0.0728\n",
            "Epoch 283/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 4.8193e-07 - mae: 5.3096e-04 - val_loss: 0.0372 - val_mae: 0.0728\n",
            "Epoch 284/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 4.7786e-07 - mae: 5.2894e-04 - val_loss: 0.0372 - val_mae: 0.0727\n",
            "Epoch 285/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 4.7729e-07 - mae: 5.2530e-04 - val_loss: 0.0372 - val_mae: 0.0727\n",
            "Epoch 286/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 4.7465e-07 - mae: 5.2668e-04 - val_loss: 0.0372 - val_mae: 0.0727\n",
            "Epoch 287/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 4.7202e-07 - mae: 5.2468e-04 - val_loss: 0.0372 - val_mae: 0.0727\n",
            "Epoch 288/600\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 4.6991e-07 - mae: 5.2423e-04 - val_loss: 0.0372 - val_mae: 0.0727\n",
            "Epoch 289/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 4.6753e-07 - mae: 5.2232e-04 - val_loss: 0.0372 - val_mae: 0.0727\n",
            "Epoch 290/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 4.6523e-07 - mae: 5.2085e-04 - val_loss: 0.0372 - val_mae: 0.0727\n",
            "Epoch 291/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 4.6283e-07 - mae: 5.2080e-04 - val_loss: 0.0372 - val_mae: 0.0727\n",
            "Epoch 292/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 4.6051e-07 - mae: 5.1637e-04 - val_loss: 0.0373 - val_mae: 0.0727\n",
            "Epoch 293/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 4.5818e-07 - mae: 5.1802e-04 - val_loss: 0.0373 - val_mae: 0.0727\n",
            "Epoch 294/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 4.5606e-07 - mae: 5.1470e-04 - val_loss: 0.0373 - val_mae: 0.0727\n",
            "Epoch 295/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 4.5291e-07 - mae: 5.1533e-04 - val_loss: 0.0372 - val_mae: 0.0727\n",
            "Epoch 296/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 4.5145e-07 - mae: 5.1163e-04 - val_loss: 0.0373 - val_mae: 0.0727\n",
            "Epoch 297/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 4.4935e-07 - mae: 5.1335e-04 - val_loss: 0.0373 - val_mae: 0.0727\n",
            "Epoch 298/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 4.4723e-07 - mae: 5.1051e-04 - val_loss: 0.0373 - val_mae: 0.0727\n",
            "Epoch 299/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 4.4521e-07 - mae: 5.0870e-04 - val_loss: 0.0373 - val_mae: 0.0727\n",
            "Epoch 300/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 4.4293e-07 - mae: 5.0849e-04 - val_loss: 0.0373 - val_mae: 0.0727\n",
            "Epoch 301/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 4.4067e-07 - mae: 5.0519e-04 - val_loss: 0.0373 - val_mae: 0.0727\n",
            "Epoch 302/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 4.3789e-07 - mae: 5.0723e-04 - val_loss: 0.0373 - val_mae: 0.0727\n",
            "Epoch 303/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 4.3690e-07 - mae: 5.0448e-04 - val_loss: 0.0373 - val_mae: 0.0727\n",
            "Epoch 304/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 4.3469e-07 - mae: 5.0188e-04 - val_loss: 0.0373 - val_mae: 0.0727\n",
            "Epoch 305/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 4.3236e-07 - mae: 5.0469e-04 - val_loss: 0.0373 - val_mae: 0.0727\n",
            "Epoch 306/600\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 4.3076e-07 - mae: 5.0014e-04 - val_loss: 0.0373 - val_mae: 0.0727\n",
            "Epoch 307/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 4.2870e-07 - mae: 4.9891e-04 - val_loss: 0.0373 - val_mae: 0.0727\n",
            "Epoch 308/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 4.2674e-07 - mae: 4.9834e-04 - val_loss: 0.0373 - val_mae: 0.0727\n",
            "Epoch 309/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 4.2377e-07 - mae: 5.0044e-04 - val_loss: 0.0373 - val_mae: 0.0726\n",
            "Epoch 310/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 4.2298e-07 - mae: 4.9597e-04 - val_loss: 0.0373 - val_mae: 0.0726\n",
            "Epoch 311/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 4.2099e-07 - mae: 4.9601e-04 - val_loss: 0.0373 - val_mae: 0.0726\n",
            "Epoch 312/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 4.1899e-07 - mae: 4.9192e-04 - val_loss: 0.0373 - val_mae: 0.0726\n",
            "Epoch 313/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 4.1702e-07 - mae: 4.9443e-04 - val_loss: 0.0373 - val_mae: 0.0726\n",
            "Epoch 314/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 4.1529e-07 - mae: 4.9063e-04 - val_loss: 0.0374 - val_mae: 0.0727\n",
            "Epoch 315/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 4.1260e-07 - mae: 4.9152e-04 - val_loss: 0.0373 - val_mae: 0.0726\n",
            "Epoch 316/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 4.1166e-07 - mae: 4.8956e-04 - val_loss: 0.0373 - val_mae: 0.0726\n",
            "Epoch 317/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 4.0972e-07 - mae: 4.8709e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 318/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 4.0805e-07 - mae: 4.8739e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 319/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 4.0624e-07 - mae: 4.8616e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 320/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 4.0436e-07 - mae: 4.8626e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 321/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 4.0251e-07 - mae: 4.8317e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 322/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 4.0090e-07 - mae: 4.8388e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 323/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.9912e-07 - mae: 4.8186e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 324/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.9731e-07 - mae: 4.7977e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 325/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.9554e-07 - mae: 4.7896e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 326/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.9378e-07 - mae: 4.8114e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 327/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.9242e-07 - mae: 4.7747e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 328/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.9010e-07 - mae: 4.7719e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 329/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.8905e-07 - mae: 4.7722e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 330/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.8624e-07 - mae: 4.7532e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 331/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.8585e-07 - mae: 4.7474e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 332/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.8449e-07 - mae: 4.7126e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 333/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.8270e-07 - mae: 4.6941e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 334/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.8121e-07 - mae: 4.7009e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 335/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.7943e-07 - mae: 4.7044e-04 - val_loss: 0.0374 - val_mae: 0.0726\n",
            "Epoch 336/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.7758e-07 - mae: 4.6677e-04 - val_loss: 0.0374 - val_mae: 0.0725\n",
            "Epoch 337/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.7562e-07 - mae: 4.6748e-04 - val_loss: 0.0374 - val_mae: 0.0725\n",
            "Epoch 338/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.7504e-07 - mae: 4.6628e-04 - val_loss: 0.0374 - val_mae: 0.0725\n",
            "Epoch 339/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.7331e-07 - mae: 4.6688e-04 - val_loss: 0.0374 - val_mae: 0.0725\n",
            "Epoch 340/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.7197e-07 - mae: 4.6457e-04 - val_loss: 0.0374 - val_mae: 0.0725\n",
            "Epoch 341/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.7035e-07 - mae: 4.6503e-04 - val_loss: 0.0374 - val_mae: 0.0725\n",
            "Epoch 342/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.6889e-07 - mae: 4.6346e-04 - val_loss: 0.0374 - val_mae: 0.0725\n",
            "Epoch 343/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.6651e-07 - mae: 4.6185e-04 - val_loss: 0.0374 - val_mae: 0.0725\n",
            "Epoch 344/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.6611e-07 - mae: 4.6042e-04 - val_loss: 0.0374 - val_mae: 0.0725\n",
            "Epoch 345/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.6457e-07 - mae: 4.6074e-04 - val_loss: 0.0374 - val_mae: 0.0725\n",
            "Epoch 346/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.6312e-07 - mae: 4.6008e-04 - val_loss: 0.0374 - val_mae: 0.0725\n",
            "Epoch 347/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.6176e-07 - mae: 4.5739e-04 - val_loss: 0.0374 - val_mae: 0.0725\n",
            "Epoch 348/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.6010e-07 - mae: 4.5916e-04 - val_loss: 0.0374 - val_mae: 0.0725\n",
            "Epoch 349/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.5785e-07 - mae: 4.5683e-04 - val_loss: 0.0374 - val_mae: 0.0725\n",
            "Epoch 350/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.5751e-07 - mae: 4.5627e-04 - val_loss: 0.0374 - val_mae: 0.0725\n",
            "Epoch 351/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.5620e-07 - mae: 4.5485e-04 - val_loss: 0.0374 - val_mae: 0.0725\n",
            "Epoch 352/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.5459e-07 - mae: 4.5527e-04 - val_loss: 0.0374 - val_mae: 0.0725\n",
            "Epoch 353/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.5336e-07 - mae: 4.5326e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 354/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.5195e-07 - mae: 4.5324e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 355/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.5072e-07 - mae: 4.5113e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 356/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.4932e-07 - mae: 4.4921e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 357/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.4789e-07 - mae: 4.4877e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 358/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.4663e-07 - mae: 4.4849e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 359/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.4528e-07 - mae: 4.4704e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 360/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.4398e-07 - mae: 4.4789e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 361/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.4248e-07 - mae: 4.4544e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 362/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.4112e-07 - mae: 4.4521e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 363/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.3948e-07 - mae: 4.4509e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 364/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.3907e-07 - mae: 4.4370e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 365/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.3779e-07 - mae: 4.4281e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 366/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.3651e-07 - mae: 4.4234e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 367/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 3.3526e-07 - mae: 4.4188e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 368/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.3401e-07 - mae: 4.4095e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 369/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.3196e-07 - mae: 4.4047e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 370/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.3173e-07 - mae: 4.3819e-04 - val_loss: 0.0375 - val_mae: 0.0725\n",
            "Epoch 371/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.2934e-07 - mae: 4.3754e-04 - val_loss: 0.0375 - val_mae: 0.0724\n",
            "Epoch 372/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.2929e-07 - mae: 4.3416e-04 - val_loss: 0.0375 - val_mae: 0.0724\n",
            "Epoch 373/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 3.2808e-07 - mae: 4.3445e-04 - val_loss: 0.0375 - val_mae: 0.0724\n",
            "Epoch 374/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.2690e-07 - mae: 4.3605e-04 - val_loss: 0.0375 - val_mae: 0.0724\n",
            "Epoch 375/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.2567e-07 - mae: 4.3317e-04 - val_loss: 0.0375 - val_mae: 0.0724\n",
            "Epoch 376/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.2452e-07 - mae: 4.3321e-04 - val_loss: 0.0375 - val_mae: 0.0724\n",
            "Epoch 377/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 3.2335e-07 - mae: 4.3325e-04 - val_loss: 0.0375 - val_mae: 0.0724\n",
            "Epoch 378/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.2227e-07 - mae: 4.3284e-04 - val_loss: 0.0375 - val_mae: 0.0724\n",
            "Epoch 379/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.2111e-07 - mae: 4.3289e-04 - val_loss: 0.0375 - val_mae: 0.0724\n",
            "Epoch 380/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.1995e-07 - mae: 4.3172e-04 - val_loss: 0.0375 - val_mae: 0.0724\n",
            "Epoch 381/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.1884e-07 - mae: 4.3065e-04 - val_loss: 0.0375 - val_mae: 0.0724\n",
            "Epoch 382/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.1773e-07 - mae: 4.2914e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 383/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 3.1664e-07 - mae: 4.2919e-04 - val_loss: 0.0376 - val_mae: 0.0725\n",
            "Epoch 384/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.1552e-07 - mae: 4.2739e-04 - val_loss: 0.0376 - val_mae: 0.0725\n",
            "Epoch 385/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.1442e-07 - mae: 4.2829e-04 - val_loss: 0.0376 - val_mae: 0.0725\n",
            "Epoch 386/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.1294e-07 - mae: 4.2860e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 387/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 3.1159e-07 - mae: 4.2672e-04 - val_loss: 0.0375 - val_mae: 0.0724\n",
            "Epoch 388/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.1133e-07 - mae: 4.2460e-04 - val_loss: 0.0375 - val_mae: 0.0724\n",
            "Epoch 389/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.1024e-07 - mae: 4.2464e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 390/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.0918e-07 - mae: 4.2426e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 391/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.0816e-07 - mae: 4.2213e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 392/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.0710e-07 - mae: 4.2214e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 393/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.0602e-07 - mae: 4.2195e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 394/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.0500e-07 - mae: 4.2050e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 395/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.0399e-07 - mae: 4.1886e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 396/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 3.0293e-07 - mae: 4.1991e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 397/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 3.0149e-07 - mae: 4.1907e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 398/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 3.0102e-07 - mae: 4.1798e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 399/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.9980e-07 - mae: 4.1592e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 400/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.9900e-07 - mae: 4.1609e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 401/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.9803e-07 - mae: 4.1599e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 402/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.9698e-07 - mae: 4.1343e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 403/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.9603e-07 - mae: 4.1426e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 404/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.9509e-07 - mae: 4.1397e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 405/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.9378e-07 - mae: 4.1395e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 406/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.9322e-07 - mae: 4.1171e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 407/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.9226e-07 - mae: 4.1219e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 408/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.9130e-07 - mae: 4.1138e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 409/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.9038e-07 - mae: 4.1027e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 410/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.8946e-07 - mae: 4.0954e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 411/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.8850e-07 - mae: 4.0959e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 412/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.8725e-07 - mae: 4.0979e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 413/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.8673e-07 - mae: 4.0649e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 414/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 2.8568e-07 - mae: 4.0864e-04 - val_loss: 0.0376 - val_mae: 0.0724\n",
            "Epoch 415/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 2.8447e-07 - mae: 4.0644e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 416/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.8411e-07 - mae: 4.0509e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 417/600\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 2.8320e-07 - mae: 4.0492e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 418/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 2.8229e-07 - mae: 4.0465e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 419/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 2.8141e-07 - mae: 4.0333e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 420/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.8047e-07 - mae: 4.0181e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 421/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.7961e-07 - mae: 4.0311e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 422/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.7869e-07 - mae: 4.0295e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 423/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 2.7789e-07 - mae: 4.0191e-04 - val_loss: 0.0377 - val_mae: 0.0724\n",
            "Epoch 424/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 2.7703e-07 - mae: 3.9990e-04 - val_loss: 0.0377 - val_mae: 0.0724\n",
            "Epoch 425/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.7602e-07 - mae: 4.0011e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 426/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 2.7495e-07 - mae: 3.9999e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 427/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 2.7464e-07 - mae: 3.9834e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 428/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 2.7312e-07 - mae: 3.9815e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 429/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 2.7303e-07 - mae: 3.9673e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 430/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 2.7218e-07 - mae: 3.9627e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 431/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 2.7132e-07 - mae: 3.9597e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 432/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 2.7050e-07 - mae: 3.9604e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 433/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.6970e-07 - mae: 3.9458e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 434/600\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 2.6827e-07 - mae: 3.9521e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 435/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 2.6813e-07 - mae: 3.9390e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 436/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 2.6729e-07 - mae: 3.9163e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 437/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 2.6651e-07 - mae: 3.9274e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 438/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 2.6569e-07 - mae: 3.9110e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 439/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 2.6442e-07 - mae: 3.9155e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 440/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 2.6340e-07 - mae: 3.9147e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 441/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.6352e-07 - mae: 3.8927e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 442/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.6272e-07 - mae: 3.8931e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 443/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.6193e-07 - mae: 3.8934e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 444/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.6116e-07 - mae: 3.8805e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 445/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.6037e-07 - mae: 3.8788e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 446/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.5961e-07 - mae: 3.8851e-04 - val_loss: 0.0376 - val_mae: 0.0723\n",
            "Epoch 447/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.5881e-07 - mae: 3.8607e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 448/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.5808e-07 - mae: 3.8745e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 449/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.5729e-07 - mae: 3.8707e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 450/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.5666e-07 - mae: 3.8530e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 451/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.5591e-07 - mae: 3.8433e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 452/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 2.5474e-07 - mae: 3.8503e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 453/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.5451e-07 - mae: 3.8383e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 454/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.5374e-07 - mae: 3.8366e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 455/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.5303e-07 - mae: 3.8341e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 456/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.5230e-07 - mae: 3.8050e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 457/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.5162e-07 - mae: 3.8182e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 458/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.5089e-07 - mae: 3.8035e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 459/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.5019e-07 - mae: 3.8029e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 460/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.4942e-07 - mae: 3.8113e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 461/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.4881e-07 - mae: 3.7981e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 462/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.4813e-07 - mae: 3.7947e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 463/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 2.4740e-07 - mae: 3.7903e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 464/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.4674e-07 - mae: 3.7816e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 465/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.4590e-07 - mae: 3.7833e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 466/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.4542e-07 - mae: 3.7735e-04 - val_loss: 0.0377 - val_mae: 0.0723\n",
            "Epoch 467/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.4440e-07 - mae: 3.7696e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 468/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.4414e-07 - mae: 3.7494e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 469/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.4298e-07 - mae: 3.7523e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 470/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.4285e-07 - mae: 3.7463e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 471/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.4215e-07 - mae: 3.7456e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 472/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.4151e-07 - mae: 3.7224e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 473/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.4088e-07 - mae: 3.7258e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 474/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.4020e-07 - mae: 3.7290e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 475/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.3955e-07 - mae: 3.7211e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 476/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.3853e-07 - mae: 3.7196e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 477/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.3833e-07 - mae: 3.7096e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 478/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.3713e-07 - mae: 3.7111e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 479/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.3708e-07 - mae: 3.7064e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 480/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.3649e-07 - mae: 3.6914e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 481/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.3584e-07 - mae: 3.6912e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 482/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.3520e-07 - mae: 3.6786e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 483/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.3456e-07 - mae: 3.6814e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 484/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 2.3395e-07 - mae: 3.6796e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 485/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.3334e-07 - mae: 3.6727e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 486/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.3273e-07 - mae: 3.6642e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 487/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.3209e-07 - mae: 3.6689e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 488/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.3123e-07 - mae: 3.6552e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 489/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.3045e-07 - mae: 3.6564e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 490/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.3037e-07 - mae: 3.6404e-04 - val_loss: 0.0377 - val_mae: 0.0722\n",
            "Epoch 491/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.2979e-07 - mae: 3.6386e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 492/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.2917e-07 - mae: 3.6249e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 493/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.2859e-07 - mae: 3.6363e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 494/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.2797e-07 - mae: 3.6352e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 495/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.2742e-07 - mae: 3.6260e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 496/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.2682e-07 - mae: 3.6182e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 497/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.2619e-07 - mae: 3.6283e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 498/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.2565e-07 - mae: 3.6154e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 499/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.2510e-07 - mae: 3.6052e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 500/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.2446e-07 - mae: 3.6170e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 501/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.2397e-07 - mae: 3.5979e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 502/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.2315e-07 - mae: 3.6037e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 503/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.2287e-07 - mae: 3.5889e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 504/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.2231e-07 - mae: 3.5879e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 505/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.2141e-07 - mae: 3.5838e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 506/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.2124e-07 - mae: 3.5660e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 507/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.2066e-07 - mae: 3.5597e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 508/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.2013e-07 - mae: 3.5654e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 509/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.1957e-07 - mae: 3.5524e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 510/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.1904e-07 - mae: 3.5543e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 511/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.1824e-07 - mae: 3.5578e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 512/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.1800e-07 - mae: 3.5503e-04 - val_loss: 0.0378 - val_mae: 0.0722\n",
            "Epoch 513/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.1708e-07 - mae: 3.5452e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 514/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.1693e-07 - mae: 3.5278e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 515/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.1641e-07 - mae: 3.5302e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 516/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.1590e-07 - mae: 3.5301e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 517/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.1534e-07 - mae: 3.5321e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 518/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.1483e-07 - mae: 3.5278e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 519/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.1432e-07 - mae: 3.5202e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 520/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.1379e-07 - mae: 3.5125e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 521/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.1298e-07 - mae: 3.5182e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 522/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.1278e-07 - mae: 3.5079e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 523/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.1228e-07 - mae: 3.5045e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 524/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.1179e-07 - mae: 3.4931e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 525/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.1128e-07 - mae: 3.4884e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 526/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.1077e-07 - mae: 3.4841e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 527/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.1026e-07 - mae: 3.4884e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 528/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.0976e-07 - mae: 3.4812e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 529/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.0924e-07 - mae: 3.4745e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 530/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.0876e-07 - mae: 3.4747e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 531/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.0812e-07 - mae: 3.4748e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 532/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.0780e-07 - mae: 3.4646e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 533/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.0733e-07 - mae: 3.4592e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 534/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 2.0682e-07 - mae: 3.4522e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 535/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.0633e-07 - mae: 3.4457e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 536/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.0584e-07 - mae: 3.4454e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 537/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.0538e-07 - mae: 3.4440e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 538/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.0488e-07 - mae: 3.4348e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 539/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.0441e-07 - mae: 3.4350e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 540/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.0382e-07 - mae: 3.4331e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 541/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.0351e-07 - mae: 3.4191e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 542/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.0281e-07 - mae: 3.4218e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 543/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.0263e-07 - mae: 3.4126e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 544/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 2.0216e-07 - mae: 3.4173e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 545/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.0166e-07 - mae: 3.4059e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 546/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.0121e-07 - mae: 3.4049e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 547/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 2.0077e-07 - mae: 3.4033e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 548/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.0008e-07 - mae: 3.4093e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 549/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.9985e-07 - mae: 3.3885e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 550/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.9942e-07 - mae: 3.3906e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 551/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.9869e-07 - mae: 3.3965e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 552/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.9854e-07 - mae: 3.3848e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 553/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.9811e-07 - mae: 3.3827e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 554/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.9765e-07 - mae: 3.3816e-04 - val_loss: 0.0378 - val_mae: 0.0721\n",
            "Epoch 555/600\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.9721e-07 - mae: 3.3725e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 556/600\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.9673e-07 - mae: 3.3804e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 557/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.9633e-07 - mae: 3.3572e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 558/600\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 1.9565e-07 - mae: 3.3619e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 559/600\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 1.9549e-07 - mae: 3.3589e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 560/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.9506e-07 - mae: 3.3524e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 561/600\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 1.9461e-07 - mae: 3.3497e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 562/600\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 1.9417e-07 - mae: 3.3517e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 563/600\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.9376e-07 - mae: 3.3412e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 564/600\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 1.9332e-07 - mae: 3.3351e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 565/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.9291e-07 - mae: 3.3382e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 566/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.9246e-07 - mae: 3.3267e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 567/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.9195e-07 - mae: 3.3288e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 568/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.9164e-07 - mae: 3.3314e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 569/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.9124e-07 - mae: 3.3267e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 570/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.9061e-07 - mae: 3.3232e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 571/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.9045e-07 - mae: 3.3087e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 572/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.9004e-07 - mae: 3.3070e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 573/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8961e-07 - mae: 3.3075e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 574/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8920e-07 - mae: 3.3038e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 575/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8880e-07 - mae: 3.2997e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 576/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8838e-07 - mae: 3.2882e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 577/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.8798e-07 - mae: 3.2914e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 578/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.8758e-07 - mae: 3.2919e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 579/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8717e-07 - mae: 3.2868e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 580/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.8677e-07 - mae: 3.2849e-04 - val_loss: 0.0379 - val_mae: 0.0721\n",
            "Epoch 581/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8633e-07 - mae: 3.2790e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 582/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8583e-07 - mae: 3.2811e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 583/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.8565e-07 - mae: 3.2662e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 584/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.8520e-07 - mae: 3.2785e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 585/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8486e-07 - mae: 3.2663e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 586/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8447e-07 - mae: 3.2616e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 587/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8387e-07 - mae: 3.2586e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 588/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8372e-07 - mae: 3.2510e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 589/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8332e-07 - mae: 3.2434e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 590/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8267e-07 - mae: 3.2543e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 591/600\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.8257e-07 - mae: 3.2481e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 592/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8218e-07 - mae: 3.2459e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 593/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8184e-07 - mae: 3.2351e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 594/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.8146e-07 - mae: 3.2343e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 595/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.8107e-07 - mae: 3.2230e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 596/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8070e-07 - mae: 3.2281e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 597/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8031e-07 - mae: 3.2256e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 598/600\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.7994e-07 - mae: 3.2216e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 599/600\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.7958e-07 - mae: 3.2138e-04 - val_loss: 0.0379 - val_mae: 0.0720\n",
            "Epoch 600/600\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.7922e-07 - mae: 3.2099e-04 - val_loss: 0.0379 - val_mae: 0.0720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUDPvaJE1wRE"
      },
      "source": [
        "## Verify\n",
        "\n",
        "Graph the models performance vs validation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxA0zCOaS35v"
      },
      "source": [
        "### Graph the loss\n",
        "\n",
        "Graph the loss to see when the model stops improving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvFNHXoQzmcM"
      },
      "source": [
        "# increase the size of the graphs. The default size is (6,4).\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
        "\n",
        "# graph the loss, the model above is configure to use \"mean squared error\" as the loss function\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'g.', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(plt.rcParams[\"figure.figsize\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3m-VpE1zOd"
      },
      "source": [
        "### Graph the loss again, skipping a bit of the start\n",
        "\n",
        "We'll graph the same data as the previous code cell, but start at index 100 so we can further zoom in once the model starts to converge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3xT7ue2zovd"
      },
      "source": [
        "# graph the loss again skipping a bit of the start\n",
        "SKIP = 100\n",
        "plt.plot(epochs[SKIP:], loss[SKIP:], 'g.', label='Training loss')\n",
        "plt.plot(epochs[SKIP:], val_loss[SKIP:], 'b.', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRjvkFQy2RgS"
      },
      "source": [
        "### Graph the mean absolute error\n",
        "\n",
        "[Mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) is another metric to judge the performance of the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBjCf1-2zx9C"
      },
      "source": [
        "# graph of mean absolute error\n",
        "mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "plt.plot(epochs[SKIP:], mae[SKIP:], 'g.', label='Training MAE')\n",
        "plt.plot(epochs[SKIP:], val_mae[SKIP:], 'b.', label='Validation MAE')\n",
        "plt.title('Training and validation mean absolute error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guMjtfa42ahM"
      },
      "source": [
        "### Run with Test Data\n",
        "Put our test data into the model and plot the predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Y0CCWJz2EK"
      },
      "source": [
        "# use the model to predict the test inputs\n",
        "predictions = model.predict(inputs_test)\n",
        "\n",
        "# print the predictions and the expected ouputs\n",
        "print(\"predictions =\\n\", np.round(predictions, decimals=3))\n",
        "print(\"actual =\\n\", outputs_test)\n",
        "\n",
        "# Plot the predictions along with to the test data\n",
        "plt.clf()\n",
        "plt.title('Training data predicted vs actual values')\n",
        "plt.plot(inputs_test, outputs_test, 'b.', label='Actual')\n",
        "plt.plot(inputs_test, predictions, 'r.', label='Predicted')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7DO6xxXVCym"
      },
      "source": [
        "# Convert the Trained Model to Tensor Flow Lite\n",
        "\n",
        "The next cell converts the model to TFlite format. The size in bytes of the model is also printed out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xn1-Rn9Cp_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edbf041e-fa19-46df-99cd-025f9f55591a"
      },
      "source": [
        "# Convert the model to the TensorFlow Lite format without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(\"gesture_model.tflite\", \"wb\").write(tflite_model)\n",
        "\n",
        "import os\n",
        "basic_model_size = os.path.getsize(\"gesture_model.tflite\")\n",
        "print(\"Model is %d bytes\" % basic_model_size)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is 148344 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykccQn7SXrUX"
      },
      "source": [
        "## Encode the Model in an Arduino Header File\n",
        "\n",
        "The next cell creates a constant byte array that contains the TFlite model. Import it as a tab with the sketch below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J33uwpNtAku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b45d064-cc75-4125-9084-2593863e00ed"
      },
      "source": [
        "!echo \"const unsigned char model[] = {\" > /content/model.h\n",
        "!cat gesture_model.tflite | xxd -i      >> /content/model.h\n",
        "!echo \"};\"                              >> /content/model.h\n",
        "\n",
        "import os\n",
        "model_h_size = os.path.getsize(\"model.h\")\n",
        "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
        "print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Header file, model.h, is 914,822 bytes.\n",
            "\n",
            "Open the side panel (refresh if needed). Double click model.h to download the file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eSkHZaLzMId"
      },
      "source": [
        "# Classifying IMU Data\n",
        "\n",
        "Now it's time to switch back to the tutorial instructions and run our new model on the Arduino Nano 33 BLE Sense to classify the accelerometer and gyroscope data.\n"
      ]
    }
  ]
}